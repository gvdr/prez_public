---
title: "How to shoot a hit."
subtitle: "A hands on introduction to classification and decision trees"
author: "Giulio Valentino Dalla Riva"
date: "2017/10/30"
output:
  xaringan::moon_reader:
    lib_dir: libs
    chakra: libs/remark-latest.min.js
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: center


```{r include=FALSE, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(fig.width=4, fig.height=3, fig.align = "center",
                      out.width = "60%", dev = "svg") 
library(tidyverse)
library(forcats)
library(ggridges)
library(magrittr)
library(skimr)
library(glue)
library(knitr)
library(kableExtra)
options(knitr.table.format = "html")
theme_set(theme_minimal())
update_geom_defaults("hline", list(size = 1.5))
update_geom_defaults("vline", list(size = 1.5))
weird <- scales::trans_new("signed_log",
       transform=function(x) sign(x)*log(abs(x)),
       inverse=function(x) sign(x)*exp(abs(x)))

set.seed(42)
```

**Today's Menu:**   
--

1.  Explain "Explaining"  
--

2.  Trees and Partitions
--
: rules and evaluation.  
--

    *  Impurity
    *  Gain
--

3.  Start to code a decision tree fitter by scratch


--

**Goals:**  
--

1.  Tell what a "decision tree" is.

--
2.  Compute the impurity of a set (for a binary attribute).  

--
3.  Use the values of a variable to partition data.


---

# Money for movies

--

**Cost**:

  $5.5e+08 for John Woo’s 2008 _Red Cliff_
  
  $1.2e+06 for Quentin Tarantino’s 1992 _Reservoir Dogs_
  
--

**Revenue**:

  $7.6e+08 for James Cameron's 2009 _Avatar_
  
  $2.7e+04 for Tony Scott's 2001 _Spy Game_

???

Setting the stage

---
class: middle

# The $64k Question(s)

1. What makes a hit?

2. Will this movie be it?

???

Present the problem: two different questions

---
class: middle

# The $64k Question(s)

1. What makes a hit? <- **Explain**

2. Will this movie be it? <- **Predict**

???

Two different tasks:
discover and explain the classification rule for observed data points
predict the classification of a new data point

---

Explaining (here) is
--

##growing a tree:

1.  We ask true/false **questions** to the data. 
2.  We **partition** the data based on the answers.
3.  Repeat 1. and 2. on the parted data.

--

##Goal:
*  partitions we can label with little uncertainty.

---

# Growing a coffe tree:

---
class: center

If the problem is tough

--

# the tough ones get the data:

```{r}
HitOrFlop <- "./HitOrFlop.Rds" %>% read_rds()
```

--

Movies (1916 to 2016) scraped from [IMDB](www.imdb.com).

---

### Hit or Flop


```{r}
HitOrFlop %>% names()
```

--

`HorF`: movies are `Hits` if

$$\log\left(\frac{\mbox{gross}}{\mbox{budget}}\right) > 0$$

or `Flops`.

---
class: centre

### Before looking at the data:

How "mixed" is it?

```{r}
HitOrFlop %>% count(HorF) 
```

---

## A balanced partition is mostly impure

The Gini impurity, $I_G(s)$, of a partition $s$ measures its mixing, and is defined as:

$$I_G(s) = 1 - \sum_i^J p_i^2 $$
were $p_i$ is the frequency of elements of classes $i \in \{1, ..., J\}$ in the partition $s$.

--

The impurity of a balanced binary partition is:

```{r}
#
#
#
#
```

---

# A fair coin flip.

Without additional insight, how would we classify?

--

```{r}
the_coin <- c("Hit","Flop") %>% as_factor()
rand_classifier <- sample(the_coin, 1168, replace = TRUE)
```

--

How bad is that?

```{r}
HitOrFlop %>%
  mutate(classified=  rand_classifier) %>% # We add our classification to the dataframe
  count(classified, HorF) # and count the combination of classified and HorF
```

---

### Fair coins are the worst

The Gini impurity, , $I_G(s)$, measures the probability of misclassification in a partition:

$$ I_G(s) = 1 - \sum_i^J p_i^2 =$$

```{r}
#
#
#
#
#
#
#
```

$$=\sum_i^J p_i (1 - p_i) $$

--

It assumes that the costs of missing a hit and running a flop are the same.


---

### How much is that mixed?

1.  Classification error
2.  Gini impurity
3.  Entropy

---

### Binary Gini

For two classes:

```{r}
gini_2 <- function(class1,class2){
  
  tot_classes <- class1 + class2 # tot of observations
  p_1 <- class1 / tot_classes # prob of labelling left
  p_2 <- class2 / tot_classes # prob of labelling right
  I_G <- 1 - ((p_1)^2) - ((p_2)^2) # Gini impurity
  
  return(I_G)
}
```

---

## The right questions

A **question** is made of:

1.  a **topic**
2.  a **condition**

---

Choose the topic.

--

### Can it be the Facebook fame of the cast?

--

```{r warning = FALSE, message = FALSE}
HitOrFlop %>% 
  ggplot(aes(x= cast_fl + 1, y= HorF, fill = HorF)) + scale_x_log10() +
  geom_density_ridges(rel_min_height= 0.005) + theme_ridges()
```

---

Choose the topic.

--

### Can it be the budget?

--

```{r warning = FALSE, message = FALSE}
HorF_budget_p <- HitOrFlop %>%
  ggplot(aes(x= budget, y= HorF, fill= HorF)) + scale_x_log10() +
  geom_density_ridges(rel_min_height= 0.015) + theme_ridges()
HorF_budget_p
```


---
Choose the question.

--

### Budget less than $1e+07?

--

```{r message = FALSE}
HorF_budget_p + geom_vline(xintercept = 1e+07)
```

---
Choose the question.

### Budget less than $1e+07?


```{r}
Part_1e07 <- if_else(
  HitOrFlop$budget < 1e+07,  # The question
  "l", # Yes -> left partition
  "r") # No -> right partition
```

---

### Good questions unmix

A question is good if it reduces the misclassification.

---

### Mixed or Unmixed?

With the $1e+07 budget threshold

```{r}
C <- HitOrFlop %>%
  mutate(classified = Part_1e07) %>%
  count(classified, HorF) %T>% print
```

???

It does look better.

---

### Gini gain

The question $q$ parted the observations into left, $l$, and right, $r$.

We measure the difference between the Gini impurity before, $I_G(s_0)$, and after the partition.

$$ \Delta I_G(q) = I_G(s_0) - \left(p_l I_G(l) + p_r I_G(r)\right) $$

We weighted the Gini impurity of $l$ and $r$ by their relative frequency, $p_l$ and $p_r$.

---

$$ \Delta I_G(q) = I_G(s_0) - \left(p_l I_G(l) + p_r I_G(r)\right) $$

--

```{r echo = FALSE}
C
```

--

```{r}
tot <- sum(C$n)
p_l <- (C$n[1] + C$n[2]) / tot  # rel frequency of "l"s
p_r <- (C$n[3] + C$n[4]) / tot # rel frequency of "r"s

old_gini <- gini_2((C$n[1] + C$n[3]), # Gini impurity before the partition
                   (C$n[2] + C$n[4]))

young_gini <- p_l * gini_2(C$n[1], C$n[2]) + # Gini impurity for the "l" partition
              p_r * gini_2(C$n[3], C$n[4]) # Gini impurity for the "l" partition

old_gini - young_gini # difference
```

---

### Was that the best question?

That $1e+07 is completely arbitrary. Other ideas?

---

### Was that the best question?

Let's turn the Gini impurity computation into function:

$$ \Delta I_G(q) = I_G(s_0) - \left(p_l I_G(l) + p_r I_G(r)\right) $$

--

```{r}
GiniGain <- function(C){
  C %<>% arrange(classified, HorF) # to assure the order in the "n" column
  
  tot <- sum(C$n)
  p_l <- (C$n[1] + C$n[2]) / tot
  p_r <- (C$n[3] + C$n[4]) / tot

  old_gini <- gini_2((C$n[1] + C$n[3]), # I_G(s_0)
                     (C$n[2] + C$n[4]))
  
  young_gini <- p_l * gini_2(C$n[1], C$n[2]) + # p_l * I_G(l)
                p_r * gini_2(C$n[3], C$n[4]) # p_r * I_G(r)
  
  DeltaGini <-  old_gini - young_gini
              
  return(DeltaGini)
}
```

---

### $2e+07

--

```{r}
C2 <- HitOrFlop %>%
  mutate(classified = if_else(budget < 2e+07, "l", "r")) %>%
  count(classified, HorF)
GiniGain(C2)
```

--

Better or worse?

```{r}
GiniGain(C2) > GiniGain(C)
```

---

### $3e+07

--

```{r}
C3 <- HitOrFlop %>%
  mutate(classified = if_else(budget < 3e+07, "l", "r")) %>%
  count(classified, HorF)
GiniGain(C3)
```

--

Better or worse?

```{r}
GiniGain(C3) > GiniGain(C2)
```

---

### How many thresholds should we try?

--

Distinct values between in the overlap area (+ 2):

--

```{r message = FALSE}
maxBudgetHits <- filter(HitOrFlop, HorF == "Hit") %$% max(budget)
minBudgetFlops <- filter(HitOrFlop, HorF == "Flop") %$% min(budget)

HorF_budget_p + geom_vline(xintercept = maxBudgetHits) +
  geom_vline(xintercept = minBudgetFlops)
```

---

Quite a lot, but the computation is light :-)

--

```{r}
Overlap <- distinct(HitOrFlop,budget) %>% # Only the distinct values matter
  filter(budget > minBudgetFlops) %>% # More than the littlest Flop
  filter(budget < maxBudgetHits) # Less than the biggest Hit
```

--

```{r}
thrs <- Overlap$budget

N <- length(thrs)
```

---

N = `r N`. Quite a lot, but the computation is light : we loop.

```{r}
Deltas <- data_frame(Threshold = numeric(N), # initialize empty dataframe
                    GiniGain = numeric(N))

for(i in seq_len(N)){ # loop over all
  thr <- thrs[i] # Pick the threshold
  Gain <- HitOrFlop %>%
   mutate(classified = if_else(budget < thr, "l", "r")) %>% # Partition at that thrs
   count(classified, HorF) %>% GiniGain() # compute Gini gain
  Deltas[i,1] <- thr; Deltas[i,2] <- Gain # store results.
}
```

.footnote[Better implementation **ARE** possible.]

---

```{r}
Deltas %>%
  qplot(Threshold, GiniGain, geom = "point", data = .) + scale_x_log10()
```

---

For budget, the best partition is:
```{r}
Deltas %>% top_n(1,GiniGain)
```

--
  
```{r}
HitOrFlop %>%
  mutate(classified = if_else(budget < 19900000, "l", "r")) %>%
  count(classified, HorF)
```
  

--

.footnote[We should do the same for **ALL** variables.]

---

### And again...

--

Iterative partitioning: let's consider **only** "l"s (or "r"s).

--

```{r warning = FALSE, message = FALSE}
HitOrFlop_l <- HitOrFlop %>% filter(budget >= 19900000)
```

---

### Repeat 1. and 2.

May it be the facebook fame of the cast??

```{r message = FALSE}
HitOrFlop_l %>% 
  ggplot(aes(y = HorF, x = cast_fl, fill = HorF)) +
  geom_density_ridges(rel_min_height = 0.015) + scale_x_log10() + theme_ridges()
```

---

### + 2

minimum budget for flops threshold

```{r}
t_min_class <- if_else(HitOrFlop$budget < minBudgetFlops, "l", "r")

HitOrFlop %>%
  mutate(classified = t_min_class) %>%
  count(classified, HorF) %>%
  rbind(list("l","Flop",0)) %>% # We add the missing class
  GiniGain()
```


---

### + 2

maximum budget for hits

```{r}
t_max_class <- if_else(HitOrFlop$budget <= maxBudgetHits, "l", "r")

HitOrFlop %>%
  mutate(classified = t_max_class) %>%
  count(classified, HorF) %>%
  rbind(list("r","Hit",0)) %>% # We add the missing class
  GiniGain()
```